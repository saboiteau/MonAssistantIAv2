import os
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration du provider LLM
PROVIDER = os.getenv("LLM_PROVIDER", "gemini").lower()

def summarize(text: str, metadata: dict) -> str:
    """
    G√©n√®re une fiche de veille au format Markdown via un LLM.

    Args:
        text (str): Le texte brut de l'article.
        metadata (dict): M√©tadonn√©es (titre, auteur, date, source).

    Returns:
        str: Le contenu Markdown de la fiche g√©n√©r√©e.
    """
    
    # Prompt syst√®me pour guider le LLM
    prompt = f"""
    Tu es mon assistant √©ditorial expert en veille technologique.
    Ta mission est de g√©n√©rer une fiche de veille structur√©e au format Markdown √† partir du texte ci-dessous.
    
    Respecte scrupuleusement ce format :
    
    # Veille : {metadata.get('title')}

    - **Source** : [{metadata.get('source')}]({metadata.get('source')})
    - **Date** : {metadata.get('date')}
    - **Auteur** : {metadata.get('author')}
    - **Tags** : #Tag1 #Tag2 #Tag3 (√† d√©duire du contenu, max 5 tags pertinents)

    ## üìù R√©sum√©
    [R√©dige un r√©sum√© structur√© de l'article en fran√ßais. Utilise des listes √† puces si n√©cessaire. Met en avant les points cl√©s.]

    ## üß† Analyse & Pense-b√™te
    [Ton analyse critique : pourquoi c'est important ? Quel impact pour moi (d√©veloppeur/manager IA) ? Id√©es d'application concr√®te.]
    
    ---
    
    Texte √† analyser :
    {text[:15000]} 
    """

    try:
        if PROVIDER == "openai":
            import openai
            openai.api_key = os.getenv("LLM_API_KEY")
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
            )
            return response.choices[0].message.content

        elif PROVIDER == "anthropic":
            import anthropic
            client = anthropic.Anthropic(api_key=os.getenv("LLM_API_KEY"))
            response = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=2000,
                temperature=0.2,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        elif PROVIDER == "gemini":
            import google.generativeai as genai
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            model = genai.GenerativeModel("gemini-1.5-flash")
            response = model.generate_content(prompt)
            return response.text

        else:
            return "Erreur : Provider LLM non support√© ou mal configur√©. V√©rifiez votre fichier .env"

    except Exception as e:
        return f"Erreur lors de la g√©n√©ration du r√©sum√© avec {PROVIDER} : {str(e)}"
